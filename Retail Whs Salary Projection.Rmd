
---
title: "Retail Whs Salary Projection"
output:
  html_document:
    toc: true
---


```{r}
library(SparkR)
library(lubridate)
library(data.table)
#library(forecast)
library(foreach)
library(SparkR)
library(sparklyr)
library(dplyr)

#shouldnt need to change this section
# Build connection string for ADW
url <- paste("jdbc:sqlserver://ashley-edw.database.windows.net;databaseName=ASHLEY_EDW;user=DataBricks;password=xsdf#$t%sdfAs")
# Set up the Blob Storage account access key in the notebook session conf.
conf <- sparkR.callJMethod(sparkR.session(), "conf")
sparkR.callJMethod(conf, "set", "fs.azure.account.key.ashleydevstorage.blob.core.windows.net", "cxG6m2fzfsood2BYII8AV9jvqgdFx3p1G/qiRNR1GoQuCrmn0FmgbiioDGJ/HTYWkC0p9Ic/5wleRTdwq/M61A==")
sparkR.callJMethod(conf, "set", "dfs.adls.oauth2.refresh.url", "https://login.microsoftonline.com/5a9d9cfd-c32e-4ac1-a9ed-fe83df4f9e4d/oauth2/token")
sparkR.callJMethod(conf, "set", "dfs.adls.oauth2.client.id", "108442fd-3527-46ea-979f-751e8f0b6b19")
sparkR.callJMethod(conf, "set", "dfs.adls.oauth2.credential", "UyffT7d/uTcfruYVT41MzxOIiMVOSiQykt4gJQd5ENw=")
sparkR.callJMethod(conf, "set", "dfs.adls.oauth2.access.token.provider.type", "ClientCredential")
sparkR.callJMethod(conf, "set", "spark.network.timeout", "1200s")
sparkR.callJMethod(conf, "set", "spark.executor.heartbeatInterval", "600s")


blobPath <- "wasbs://databricks@ashleydevstorage.blob.core.windows.net/tempDirs"
#############################################################

df <- read.df(
  source = "com.databricks.spark.sqldw",
  url = url,
  forward_spark_azure_storage_credentials = "true",
  query = "SELECT 
a.IFMS_PERIOD AS [Year_Month],
TRIM(a.[IFMS_UNIT]) AS [IFMS_UNIT],SUM([Amount]) AS [Amount],
SUM([Headcount]) AS [Headcount],
CASE 
	 WHEN RIGHT(a.IFMS_PERIOD,2)=03 THEN 5
	 WHEN RIGHT(a.IFMS_PERIOD,2)=06 THEN 5
	 WHEN RIGHT(a.IFMS_PERIOD,2)=09 THEN 5
	 WHEN RIGHT(a.IFMS_PERIOD,2)=12 THEN 5
     ELSE 4 END [Weeks]
FROM(SELECT  [IFMS_PERIOD],[IFMS_UNIT],[IFMS_NATURE],[IFMS_AMOUNT]
	  ,CASE WHEN [IFMS_NATURE]='5020' THEN [IFMS_AMOUNT] ELSE 0 END AS [Amount],
	  CASE WHEN [IFMS_NATURE]='9506' THEN [IFMS_AMOUNT] ELSE 0 END AS [Headcount] 
	  FROM [IFM_ENH].[GL_Summary] 
	  WHERE IFMS_UNCODE IN('64405020','64515020','64805020','65405020','67005020','67095020','69005020','62005020','75595020',
	                       '75045020','75515020','61005020','64409506','64519506','64809506','65409506','67009506','62009506',
						   '75599506','67099506','69009506','75049506','75519506','61009506')
      AND IFMS_PERIOD>=1801) a 
	  GROUP BY a.IFMS_PERIOD ,
	  a.[IFMS_UNIT],
	  CASE WHEN LEFT(a.IFMS_PERIOD,1)='1' THEN 4 ELSE 5 END 
	  
	  HAVING SUM([Amount])>0 AND SUM([Headcount]) >0 ",
  tempDir = "wasbs://databricks@ashleydevstorage.blob.core.windows.net/tempDirs")


df_current <- read.df(
  source = "com.databricks.spark.sqldw",
  url = url,
  forward_spark_azure_storage_credentials = "true",
  query = "SELECT 
       TRIM(b.Department) AS Department,
       b.[EMP_FACILITY_NUM],
       b.Current_HC,
       b.Date_Run,
       CASE
           WHEN RIGHT(b.Date_Run, 2) = 03 THEN
               5
           WHEN RIGHT(b.Date_Run, 2) = 06 THEN
               5
           WHEN RIGHT(b.Date_Run, 2) = 09 THEN
               5
           WHEN RIGHT(b.Date_Run, 2) = 12 THEN
               5
           ELSE
               4
       END [Weeks]
FROM
(
    SELECT a.EMP_DEPARTMENT_RPTG AS [Department],
           COUNT(DISTINCT a.EMP_EEID) AS [Current_HC],
           (a.[TBL_SNAPSHOT_RUN_MONTH]) AS [Date_Month],
           a.[EMP_FACILITY_NUM]
		   ,a.FiscalMonth AS [Date_Run]
    FROM
    (
        SELECT EMP_EEID,
               TBL_SNAPSHOT_RUN_MONTH,
			   b.FiscalMonth,
               EMP_FACILITY_NUM,
               EMP_DEPARTMENT_RPTG,
               EMP_STATUS,
               EMP_SALARY_HOURLY_FLAG,
               TBL_SNAPSHOT_RUN_DATE
        FROM [HR_Enh].[EmployeeHistory] a
		LEFT JOIN [Enterprise_DW].[DimDate] b ON a.TBL_SNAPSHOT_RUN_DATE=b.DateID
        WHERE EMP_DEPARTMENT_RPTG IN ( '6540', '6700', '6100', '6200', '6900', '7504', '7551', '6709', '6440', '6480',
                                       '6451', '7559'
                                     )
              AND EMP_STATUS = 'A'
              AND CAST([TBL_SNAPSHOT_RUN_DATE] AS DATE) =
              (
                  SELECT DATEADD(DAY, -7, MAX(CAST([TBL_SNAPSHOT_FISCAL_WEEK_DATE] AS DATE)))
                  FROM [HR_Enh].[EmployeeHistory]
              )
              AND EMP_SALARY_HOURLY_FLAG = 'S'
    ) a
    GROUP BY a.EMP_DEPARTMENT_RPTG,
             a.[TBL_SNAPSHOT_RUN_MONTH],
             a.[EMP_FACILITY_NUM]
			 ,a.FiscalMonth
) b ",
  tempDir = "wasbs://databricks@ashleydevstorage.blob.core.windows.net/tempDirs")

```


```{r}
# Data Preprocessing and feature engineering 

df<-as.data.frame(df)
df<-df[c(-98,-121),]
df$IFMS_UNIT<-as.factor(df$IFMS_UNIT)

df_current<-as.data.frame(df_current)
df_current$Department<-as.factor(df_current$Department)


sc<-spark_connect(method="databricks")
df<-copy_to(sc,df,overwrite = TRUE)
df_current<-copy_to(sc,df_current,overwrite = TRUE)

```


```{r}
# Model development and Evaluation 

df1<-df%>%select(IFMS_UNIT,Amount,Headcount,Weeks)%>%mutate(log_Amount=log(Amount),log_Head=log(Headcount))
df2<-df_current%>%select(Department,EMP_FACILITY_NUM,Date_Run,Current_HC,Weeks)%>%mutate(IFMS_UNIT=Department,log_Head=log(Current_HC))%>%
                                         select(EMP_FACILITY_NUM,Date_Run,Current_HC,log_Head,IFMS_UNIT,Weeks)
lr<-ml_linear_regression(df1,log_Amount~log_Head+IFMS_UNIT+Weeks)


# Prediction and storing the results as a dataframe
prediction<-ml_predict(lr,df2)
pred<-prediction%>%mutate(prediction=exp(prediction))

pd<-as.data.frame(pred)

```


```{r}

sdf <- as.DataFrame(pd)
write.df(sdf,path = "adl://ashleydevlake.azuredatalakestore.net/Raw_Zone/Retail/DataBricks/WarehouseSalaryPosition",source = "csv", mode = "overwrite",schema="true",header="true")
```

